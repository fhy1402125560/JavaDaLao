# 1.Redis和MySQL一致性怎么保证？

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

- **强一致性**：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
- **弱一致性**：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
- **最终一致性**：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型

## 三种缓存读写策略

### Cache Aside Pattern（旁路缓存模式）

**写**：

- 先更新 db
- 然后直接删除 cache 。

**读** :

- 从 cache 中读取数据，读取到就直接返回
- cache 中读取不到的话，就从 db 中读取数据返回
- 再把数据放到 cache 中。

### Read-Through/Write-Through（读写穿透）

**写（Write Through）：**

- 先查 cache，cache 中不存在，直接更新 db。
- cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（**同步更新 cache 和 db**）。

**读(Read Through)：**

- 从 cache 中读取数据，读取到就直接返回 。
- 读取不到的话，先从 db 加载，写入到 cache 后返回响应。

### Write Behind Pattern（异步缓存写入）

Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 db 的读写。

但是，两个又有很大的不同：**Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。**

# 2.用过线程池吗？讲讲线程池的参数？

线程池的好处：

- **降低资源消耗**。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
- **提高响应速度**。当任务到达时，任务可以不需要等到线程创建就能立即执行。
- **提高线程的可管理性**。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。



**`ThreadPoolExecutor` 3 个最重要的参数：**

- **`corePoolSize` :** 任务队列未达到队列容量时，最大可以同时运行的线程数量。
- **`maximumPoolSize` :** 任务队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。
- **`workQueue`:** 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。

`ThreadPoolExecutor`其他常见参数 :

- **`keepAliveTime`**:线程池中的线程数量大于 `corePoolSize` 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 `keepAliveTime`才会被回收销毁。
- **`unit`** : `keepAliveTime` 参数的时间单位。
- **`threadFactory`** :executor 创建新线程的时候会用到。
- **`handler`** :饱和策略。

# 3.Java怎么创建线程池？如何查看线程池的状态？

- **通过`ThreadPoolExecutor`构造函数来创建**
- **通过 `Executor` 框架的工具类 `Executors` 来创建**



- `executor.getActiveCount()` 获取线程池正在运行的任务数量。
- `executor.getTaskCount()` 获取线程池已提交的任务数量。
- `executor.getCompletedTaskCount()` 获取线程池已完成的任务数量。
- `executor.getQueue().size()` 获取线程池队列中的任务数量。

# 4.讲一下Hashmap的底层原理以及扩容过程

HashMap底层是基于哈希表+红黑树结构实现的，key值存放在哈希表的数组桶中，Value值存放在对应桶的链表下，当链表长度大于阈值（默认为8）时，会调用treeifyBin()方法，这个方法会根据HashMap数组决定是否转换为红黑树。只有当数组长度大于等于64的情况下才会执行转换红黑树操作，否则就只是执行resize()方法

- 首先判断扩容，插入数据后，如果容量大于threshold (阈值)（容量*负载因子），则进行扩容；
- 获取当前容量，若当前容量大于0，并且大于MAXIMUM_CAPACITY = 1 << 30，则不进行扩容，将 threshold = Integer.MAX_VALUE进行赋值。
- 否则创建新的数组，将容量进行两倍扩大，threshold也进行两倍扩容
- 扩容后将旧数组的元素进行迁移到新数组上，通过尾插法进行插入到新数组中新的位置。

# 5.线程池如何设置守护线程？

调用Thread.setDaemon()方法

# 6.讲讲Java锁升级的过程

无锁 -> 偏向锁 -> 轻量级锁 -> 重量级锁



![image-20240114102716285](E:\笔记\面试题\assets\image-20240114102716285.png)

![image-20240114102656871](E:\笔记\面试题\assets\image-20240114102656871.png)

![image-20240114102836310](E:\笔记\面试题\assets\image-20240114102836310.png)

![image-20240114102853205](E:\笔记\面试题\assets\image-20240114102853205.png)

![image-20240114102910629](E:\笔记\面试题\assets\image-20240114102910629.png)

# 7.Concurrenthashmap如何实现互斥的

###### JDK1.8之前：

首先将数据分为一段一段（这个“段”就是 `Segment`）的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。

**`ConcurrentHashMap` 是由 `Segment` 数组结构和 `HashEntry` 数组结构组成**。

`Segment` 继承了 `ReentrantLock`,所以 `Segment` 是一种可重入锁，扮演锁的角色。`HashEntry` 用于存储键值对数据。

一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组，`Segment` 的个数一旦**初始化就不能改变**。 `Segment` 数组的大小默认是 16，也就是说默认可以同时支持 16 个线程并发写。

`Segment` 的结构和 `HashMap` 类似，是一种数组和链表结构，一个 `Segment` 包含一个 `HashEntry` 数组，每个 `HashEntry` 是一个链表结构的元素，每个 `Segment` 守护着一个 `HashEntry` 数组里的元素，当对 `HashEntry` 数组的数据进行修改时，必须首先获得对应的 `Segment` 的锁。也就是说，对同一 `Segment` 的并发写入会被阻塞，不同 `Segment` 的写入是可以并发执行的。

###### JDK1.8之后：

`ConcurrentHashMap` 取消了 `Segment` 分段锁，采用 `Node + CAS + synchronized` 来保证并发安全。数据结构跟 `HashMap` 1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）。

Java 8 中，锁粒度更细，`synchronized` 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，就不会影响其他 Node 的读写，效率大幅提升



# 8.G1垃圾回收的过程

1. 初始标记

2. 并发标记

3. 最终标记

4. 筛选回收

   ![G1 收集器](E:\笔记\面试题\assets\g1-garbage-collector-1704858740953-3.png)

**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)** 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。
